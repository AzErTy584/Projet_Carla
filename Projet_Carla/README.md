# Projet PPO - Conduite Autonome avec CARLA

## ğŸ“‹ Description

Ce projet implÃ©mente un systÃ¨me d'apprentissage par renforcement pour la conduite autonome dans le simulateur CARLA. Il utilise l'algorithme **Proximal Policy Optimization (PPO)** pour entraÃ®ner un vÃ©hicule Ã  naviguer de maniÃ¨re autonome tout en respectant les rÃ¨gles de conduite.

## ğŸ¯ Objectifs

- Apprendre Ã  un vÃ©hicule Ã  conduire de maniÃ¨re autonome
- Maintenir le vÃ©hicule au centre de sa voie
- Ã‰viter les collisions avec les obstacles
- Maintenir une vitesse appropriÃ©e
- Progresser vers une destination dÃ©finie

## ğŸ—ï¸ Architecture du Projet

```
projet/
â”‚
â”œâ”€â”€ simulation_V6.py      # Environnement CARLA (gym)
â”œâ”€â”€ model_PPO_V6.py       # ModÃ¨le PPO Actor-Critic
â”œâ”€â”€ main_V6.py            # Script d'entraÃ®nement principal
â”œâ”€â”€ requirements.txt      # DÃ©pendances Python
â””â”€â”€ V0/
    â””â”€â”€ model_checkpoint_PPO.pth  # Checkpoints du modÃ¨le
```

## ğŸš€ Installation

### PrÃ©requis

- **CARLA Simulator** (version 0.9.13 ou supÃ©rieure)
- **Python** 3.8+
- **CUDA** (optionnel, pour GPU)
- **8 Go RAM minimum** (16 Go recommandÃ©)

### Ã‰tapes d'Installation

1. **Installer CARLA**
   ```bash
   # TÃ©lÃ©charger depuis https://github.com/carla-simulator/carla/releases
   # Extraire et noter le chemin d'installation
   ```

2. **Configurer l'environnement Python**
   ```bash
   # CrÃ©er un environnement virtuel
   python -m venv carla_env
   source carla_env/bin/activate  # Linux/Mac
   # ou
   carla_env\Scripts\activate  # Windows
   ```

3. **Installer les dÃ©pendances**
   ```bash
   pip install -r requirements.txt
   ```

4. **Configurer CARLA PythonAPI**
   ```bash
   # Ajouter le chemin vers CARLA PythonAPI
   export PYTHONPATH=$PYTHONPATH:/path/to/CARLA/PythonAPI/carla
   ```

## ğŸ® Utilisation

### DÃ©marrage du Serveur CARLA

```bash
# Dans le rÃ©pertoire CARLA
./CarlaUE4.sh -RenderOffScreen -carla-port=2000 -nosound
# ou sur Windows
CarlaUE4.exe -RenderOffScreen -carla-port=2000 -nosound
```

### EntraÃ®nement du ModÃ¨le

```bash
python main_V6.py
```

Le script va :
- Charger un modÃ¨le existant si disponible (depuis `V0/model_checkpoint_PPO.pth`)
- CrÃ©er un nouveau modÃ¨le sinon
- EntraÃ®ner pendant 300 Ã©pisodes
- Sauvegarder les checkpoints tous les 3 Ã©pisodes
- GÃ©nÃ©rer un graphique des rÃ©compenses (`training_rewards_PPO.png`)

### ParamÃ¨tres Configurables

Dans `main_V6.py` :
- `num_episodes = 300` : Nombre d'Ã©pisodes d'entraÃ®nement
- `learning_rate = 3e-4` : Taux d'apprentissage
- `episode % 3 == 0` : FrÃ©quence de sauvegarde

Dans `simulation_V6.py` :
- `Num_sectors_lidar=16` : RÃ©solution du LIDAR
- `lidar_range=50` : PortÃ©e du LIDAR (mÃ¨tres)
- `max_obstacles=3` : Nombre d'obstacles maximum

## ğŸ“Š Composants du SystÃ¨me de RÃ©compense

Le systÃ¨me de rÃ©compense est multi-objectifs et Ã©quilibre plusieurs aspects :

| Composant | Poids | Description |
|-----------|-------|-------------|
| **Base** | +0.01 | RÃ©compense de survie |
| **Lane Keeping** | +0.25 Ã  +0.55 | Maintien dans la voie |
| **Consistency Bonus** | +0.05 Ã  +0.2 | Conduite stable prolongÃ©e |
| **Speed** | Â±0.5 | Vitesse appropriÃ©e |
| **Exploration** | +0.1 max | Distance parcourue |
| **Collision** | -500 | PÃ©nalitÃ© collision |
| **Immobility** | -0.001/step | PÃ©nalitÃ© immobilitÃ© |
| **Off-Road** | Variable | Conduite hors route |

## ğŸ§  Architecture du RÃ©seau de Neurones

### EntrÃ©es (39 dimensions)
- **LIDAR** : 32 secteurs (distances normalisÃ©es)
- **Collision** : 1 intensitÃ©
- **Speed** : 1 vitesse actuelle
- **Lane Offset** : 1 dÃ©calage latÃ©ral
- **Lane Angle** : 1 angle avec la voie
- **Goal Direction** : 2 vecteur directionnel
- **Goal Distance** : 1 distance Ã  l'objectif

### Architecture
```
Input (39) â†’ FC(256) â†’ ReLU â†’ FC(256) â†’ ReLU
                                        â†“
                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                        â†“                               â†“
                   Actor (3)                       Critic (1)
              (steer, throttle, brake)            (value estimate)
```

### Sorties
- **Actions** : 3 valeurs continues [0, 1]
  - Steering (direction)
  - Throttle (accÃ©lÃ©ration)
  - Brake (freinage)

## ğŸ“ˆ Monitoring de l'EntraÃ®nement

Le script gÃ©nÃ¨re automatiquement :
- **Logs console** : RÃ©compenses et composants par Ã©pisode
- **Graphique** : `training_rewards_PPO.png` avec toutes les composantes
- **Checkpoints** : SauvegardÃ©s rÃ©guliÃ¨rement dans `V0/`

## ğŸ”§ DÃ©pannage

### ProblÃ¨me : "Connection refused"
```bash
# VÃ©rifier que CARLA est lancÃ©
ps aux | grep Carla  # Linux/Mac
tasklist | findstr Carla  # Windows
```

### ProblÃ¨me : "CUDA Out of Memory"
```python
# Dans main_V6.py, rÃ©duire la taille du batch ou utiliser CPU
device = torch.device("cpu")
```

### ProblÃ¨me : Le vÃ©hicule reste immobile
- VÃ©rifier que `min_throttle = 0` dans `simulation_V6.py`
- Le systÃ¨me force automatiquement le throttle si vitesse < 1 m/s

## ğŸ“ Licence

Ce projet est fourni Ã  des fins Ã©ducatives et de recherche.

## ğŸ‘¥ Contributeurs

Projet de recherche en apprentissage par renforcement appliquÃ© Ã  la conduite autonome.

## ğŸ“š RÃ©fÃ©rences

- [CARLA Simulator](https://carla.org/)
- [PPO Algorithm (Schulman et al., 2017)](https://arxiv.org/abs/1707.06347)
- [PyTorch Documentation](https://pytorch.org/docs/)
