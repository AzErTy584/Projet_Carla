# Documentation Technique - Projet PPO CARLA V6

## Table des Mati√®res

1. [Vue d'Ensemble](#vue-densemble)
2. [Architecture Syst√®me](#architecture-syst√®me)
3. [Composants D√©taill√©s](#composants-d√©taill√©s)
4. [Algorithme PPO](#algorithme-ppo)
5. [Environnement CARLA](#environnement-carla)
6. [Fonction de R√©compense](#fonction-de-r√©compense)
7. [Optimisations et Consid√©rations](#optimisations-et-consid√©rations)

---

## 1. Vue d'Ensemble

### 1.1 Objectif du Projet

D√©velopper un syst√®me de conduite autonome bas√© sur l'apprentissage par renforcement profond (Deep RL) utilisant l'algorithme Proximal Policy Optimization (PPO) dans l'environnement de simulation CARLA.

### 1.2 Stack Technique

- **Langage** : Python 3.8+
- **Framework RL** : PyTorch 2.0+
- **Simulateur** : CARLA 0.9.13+
- **API Gym** : Gymnasium
- **Acc√©l√©ration** : CUDA (optionnel)

### 1.3 M√©triques de Performance

- **R√©compense totale** : Somme des r√©compenses par √©pisode
- **Distance parcourue** : Distance maximale depuis le spawn
- **Lane keeping streak** : Nombre de steps cons√©cutifs bien centr√©s
- **Collisions** : Intensit√© et fr√©quence
- **Vitesse moyenne** : Performance de d√©placement

---

## 2. Architecture Syst√®me

### 2.1 Diagramme de Flux

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  CARLA Server   ‚îÇ
‚îÇ   (Town01)      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ    CarlaEnv (simulation_V6)     ‚îÇ
‚îÇ  ‚Ä¢ Sensors (LIDAR, Collision)   ‚îÇ
‚îÇ  ‚Ä¢ State Extraction             ‚îÇ
‚îÇ  ‚Ä¢ Reward Calculation           ‚îÇ
‚îÇ  ‚Ä¢ Episode Management           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   PPOActorCritic (model_PPO)    ‚îÇ
‚îÇ  ‚Ä¢ Shared Backbone (256‚Üí256)    ‚îÇ
‚îÇ  ‚Ä¢ Actor Head (mean + std)      ‚îÇ
‚îÇ  ‚Ä¢ Critic Head (value)          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ    Training Loop (main_V6)      ‚îÇ
‚îÇ  ‚Ä¢ Rollout Collection           ‚îÇ
‚îÇ  ‚Ä¢ GAE Computation              ‚îÇ
‚îÇ  ‚Ä¢ PPO Update                   ‚îÇ
‚îÇ  ‚Ä¢ Checkpoint Management        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### 2.2 Pipeline de Donn√©es

```
CARLA World State ‚Üí Sensors ‚Üí Feature Extraction ‚Üí Neural Network ‚Üí Actions ‚Üí Vehicle Control ‚Üí CARLA
                                                           ‚Üì
                                                    Reward Signal
                                                           ‚Üì
                                                    Buffer Storage
                                                           ‚Üì
                                                     PPO Training
```

---

## 3. Composants D√©taill√©s

### 3.1 simulation_V6.py - Environnement CARLA

#### 3.1.1 Classe CarlaEnv

**H√©ritage** : `gym.Env`

**Responsabilit√©s** :
- Connexion au serveur CARLA
- Gestion du v√©hicule et des capteurs
- Calcul des observations
- Calcul des r√©compenses
- Gestion du cycle de vie des √©pisodes

#### 3.1.2 Capteurs Impl√©ment√©s

**1. LIDAR (sensor.lidar.ray_cast)**
```python
Param√®tres:
- Channels: 1 (plan horizontal)
- Range: 50 m√®tres
- Points par seconde: 56000
- Secteurs: 32 (16 * 2 bins)
- Rotation: -90¬∞ √† +90¬∞ en yaw

Traitement:
- Binning par secteur angulaire
- Minimum des distances par secteur
- Normalisation [0, lidar_range]
```

**2. Collision (sensor.other.collision)**
```python
Donn√©es:
- Normal impulse (vecteur 3D)
- Intensit√© = ||normal_impulse||

Seuils:
- Collision critique: > 75.0
- P√©nalit√© d√©clench√©e: > 50.0
```

**3. Position & Orientation**
```python
Sources:
- Vehicle.get_transform()
- World.get_map().get_waypoint()

Calculs:
- Lane offset: Distance perpendiculaire au centre de voie
- Lane angle: Diff√©rence d'orientation v√©hicule/voie
- Goal direction: Vecteur normalis√© vers destination
- Goal distance: Distance euclidienne 2D
```

#### 3.1.3 Espace d'Observation

```python
observation_space = {
    "lidar": Box(0, 50, shape=(32,)),           # Distances LIDAR
    "collision": Box(0, ‚àû, shape=(1,)),         # Intensit√© collision
    "speed": Box(0, 200, shape=(1,)),           # Vitesse (km/h)
    "position": Box(-‚àû, ‚àû, shape=(2,)),         # Position (x, y)
    "lane_offset": Box(-‚àû, ‚àû, shape=(1,)),      # Offset lat√©ral normalis√©
    "lane_angle": Box(-œÄ, œÄ, shape=(1,)),       # Angle avec voie normalis√©
    "goal_direction": Box(-1, 1, shape=(2,)),   # Direction but (x, y)
    "goal_distance": Box(0, ‚àû, shape=(1,))      # Distance au but
}

Total: 39 dimensions
```

#### 3.1.4 Gestion des Obstacles

```python
Param√®tres:
- max_obstacles = 3
- spawn_distance = [30, 60] m√®tres
- respawn_interval = 100 steps

Logique:
- Spawn al√©atoire devant le v√©hicule
- Vitesse constante (autopilot)
- Despawn si hors de port√©e
- Respawn p√©riodique
```

---

### 3.2 model_PPO_V6.py - R√©seau de Neurones PPO

#### 3.2.1 Architecture PPOActorCritic

```python
class PPOActorCritic(nn.Module):
    
    Couches:
    ‚îú‚îÄ‚îÄ Backbone (partag√©)
    ‚îÇ   ‚îú‚îÄ‚îÄ Linear(39 ‚Üí 256)
    ‚îÇ   ‚îú‚îÄ‚îÄ ReLU
    ‚îÇ   ‚îú‚îÄ‚îÄ Linear(256 ‚Üí 256)
    ‚îÇ   ‚îî‚îÄ‚îÄ ReLU
    ‚îÇ
    ‚îú‚îÄ‚îÄ Actor
    ‚îÇ   ‚îú‚îÄ‚îÄ actor_mean: Linear(256 ‚Üí 3)
    ‚îÇ   ‚îî‚îÄ‚îÄ actor_log_std: Parameter(3)
    ‚îÇ
    ‚îî‚îÄ‚îÄ Critic
        ‚îî‚îÄ‚îÄ critic: Linear(256 ‚Üí 1)
```

**Initialisation** :
```python
# Actor mean
nn.init.xavier_uniform_(actor_mean.weight, gain=0.1)
nn.init.zeros_(actor_mean.bias)

# Actor std
actor_log_std = ones(3) * 0.5  # std initiale ‚âà 1.65
```

#### 3.2.2 Forward Pass

```python
def forward(state_tensor):
    """
    Input: (B, 39)
    
    1. Backbone: (B, 39) ‚Üí (B, 256)
    2. Actor: (B, 256) ‚Üí (B, 3) mean, (3,) std
    3. Critic: (B, 256) ‚Üí (B, 1) ‚Üí (B,) value
    
    Output: mean, std, value
    """
```

#### 3.2.3 Sampling d'Actions

```python
def act(state_tensor):
    """
    Distribution: Normal(mean, std)
    
    1. Sample: raw_action ~ N(Œº, œÉ)
    2. Transform: action = sigmoid(raw_action) ‚àà [0, 1]
    3. Log prob: log œÄ(raw_action|state)
    
    Raison sigmoid:
    - Borne les actions dans [0, 1]
    - N√©cessaire pour CARLA (throttle, brake, steer)
    """
```

**Note importante** : Le code utilise `sigmoid` pour la transformation, mais le commentaire et `evaluate()` mentionnent `tanh`. Il y a une incoh√©rence √† corriger :

```python
# Dans act():
action = torch.sigmoid(raw_action)  # Actuel

# Dans evaluate():
raw_action = torch.atanh(...)  # Inverse de tanh, pas sigmoid!

# CORRECTION N√âCESSAIRE dans evaluate():
raw_action = torch.logit(torch.clamp(action, 1e-7, 1-1e-7))
```

#### 3.2.4 RolloutBuffer

```python
class RolloutBuffer:
    """
    Stockage temporaire des exp√©riences d'un √©pisode
    
    Donn√©es:
    - states: List[Tensor(39)]
    - actions: List[Tensor(3)]
    - log_probs: List[float]
    - rewards: List[float]
    - dones: List[bool]
    - values: List[float]
    """
```

---

### 3.3 Algorithme PPO

#### 3.3.1 Generalized Advantage Estimation (GAE)

```python
def compute_gae(rewards, values, dones, Œ≥=0.99, Œª=0.95):
    """
    Calcul r√©cursif des avantages:
    
    Œ¥‚Çú = r‚Çú + Œ≥¬∑V(s‚Çú‚Çä‚ÇÅ)¬∑(1-d‚Çú) - V(s‚Çú)
    A‚Çú = Œ¥‚Çú + Œ≥¬∑Œª¬∑(1-d‚Çú)¬∑A‚Çú‚Çä‚ÇÅ
    
    Returns: R‚Çú = A‚Çú + V(s‚Çú)
    
    Param√®tres:
    - Œ≥ (gamma): Discount factor (importance du futur)
    - Œª (lambda): GAE parameter (bias-variance tradeoff)
    """
```

**Intuition** :
- **Œ≥ = 0.99** : Valorise fortement les r√©compenses futures
- **Œª = 0.95** : Balance entre low bias (Œª‚Üí1) et low variance (Œª‚Üí0)

#### 3.3.2 PPO Update

```python
def ppo_update(model, optimizer, buffer, 
               clip_eps=0.2, value_coef=0.5, entropy_coef=0.05, epochs=10):
    """
    Objectif PPO:
    L = L_CLIP + c‚ÇÅ¬∑L_VF - c‚ÇÇ¬∑H
    
    O√π:
    - L_CLIP: Clipped surrogate objective
    - L_VF: Value function loss
    - H: Entropy bonus
    
    Hyperparam√®tres:
    - clip_eps (Œµ): 0.2 ‚Üí ratio ‚àà [0.8, 1.2]
    - value_coef (c‚ÇÅ): 0.5
    - entropy_coef (c‚ÇÇ): 0.05
    - epochs: 10 (mini-batches sur les m√™mes donn√©es)
    """
```

**Clipped Surrogate Objective** :
```python
ratio = exp(log œÄ_new - log œÄ_old)
surr1 = ratio ¬∑ A
surr2 = clip(ratio, 1-Œµ, 1+Œµ) ¬∑ A
L_CLIP = -min(surr1, surr2)
```

**Avantages de PPO** :
- Stabilit√© : Clipping emp√™che les mises √† jour trop agressives
- Efficacit√© : R√©utilise les donn√©es (10 epochs)
- Simplicit√© : Pas de contrainte KL explicite

---

## 4. Environnement CARLA

### 4.1 Configuration du Monde

```python
World: Town01
Mode: Synchronous (fixed_delta_seconds = 0.05s ‚Üí 20 FPS)
Weather: Cloudiness=0, Precipitation=0, Sun=45¬∞
```

### 4.2 V√©hicule

```python
Blueprint: vehicle.tesla.model3
Spawn: Al√©atoire parmi les spawn points
Destination: carla.Location(x=100, y=50, z=0)
```

### 4.3 Boucle de Simulation

```python
1. apply_control(throttle, brake, steer)
2. world.tick() ou wait_for_tick()
3. update_obstacles()
4. read_sensors() ‚Üí observation
5. compute_reward() ‚Üí reward, done
```

---

## 5. Fonction de R√©compense

### 5.1 Syst√®me Multi-Objectifs

```python
total_reward = Œ£ [
    base_reward,
    lane_reward,
    consistency_bonus,
    speed_reward,
    exploration_reward,
    collision_penalty,
    immobility_penalty,
    off_road_penalty,
    off_road_termination_penalty
]

Clipping: [-150, 150]
```

### 5.2 D√©tail des Composants

#### 5.2.1 Base Reward
```python
base_reward = 0.01
# R√©compense minimale pour chaque step surv√©cu
```

#### 5.2.2 Lane Keeping Reward (Progressif)

```python
Seuils:
- excellent_offset ‚â§ 0.1
- good_offset ‚â§ 0.3
- bad_offset ‚â§ 0.7
- critical_offset ‚â§ 0.9

Seuils d'angle:
- excellent_angle ‚â§ 0.05
- good_angle ‚â§ 0.15
- bad_angle ‚â§ 0.4
- critical_angle ‚â§ 0.7

R√©compenses:
- Excellent: 0.25 + min(streak * 0.02, 0.3) ‚Üí max 0.55
- Good: 0.1 + min(streak * 0.01, 0.1) ‚Üí max 0.2
- Acceptable: 0.025
- Mauvais: -2¬∑offset¬≤ - 3¬∑angle¬≤
```

**Streak System** :
```python
if excellent or good:
    lane_keeping_streak += 1
    off_road_steps = 0
else:
    lane_keeping_streak = max(0, streak - 2)  # D√©croissance rapide
    if off_road:
        off_road_steps += 1
```

#### 5.2.3 Consistency Bonus

```python
if streak ‚â• 50: bonus = 0.2
elif streak ‚â• 20: bonus = 0.1
elif streak ‚â• 10: bonus = 0.05
else: bonus = 0.0
```

#### 5.2.4 Speed Reward (Conditionnel)

```python
if good_keeping or excellent_keeping:
    speed_reward = 0.5 ¬∑ clip(speed/10, 0, 1)
else:
    speed_reward = -0.05 ¬∑ clip(speed/10, 0, 1)  # P√©nalit√©
```

**Logique** : R√©compenser la vitesse uniquement si bien centr√©

#### 5.2.5 Exploration Reward

```python
distance_from_spawn = ||position - spawn_location||
exploration_reward = min(distance_from_spawn * 0.05, 0.1)
```

#### 5.2.6 P√©nalit√©s Critiques

```python
# Collision
if collision > 0:
    penalty = -500.0
    
# Immobilit√©
penalty = -0.001 ¬∑ stationary_steps

# Off-Road
penalty = -0.1 ¬∑ (off_road_steps^1.5)

# Termination Off-Road
if off_road_steps > 30 or offset > 0.95:
    penalty = -250.0
    done = True
```

### 5.3 Conditions de Terminaison

```python
done = (
    collision > 50.0 or
    elapsed ‚â• 5000 steps or
    off_road_steps > 30 or
    lane_offset > 0.95
)
```

---

## 6. Optimisations et Consid√©rations

### 6.1 Gestion M√©moire GPU

```python
def safe_gpu_cleanup():
    """
    Appel√© apr√®s chaque √©pisode
    """
    torch.cuda.empty_cache()
    torch.cuda.synchronize()
    gc.collect()
```

**Raisons** :
- CARLA + PyTorch peuvent saturer la VRAM
- Pr√©vient les `CUDA Out of Memory`
- Crucial pour entra√Ænements longs

### 6.2 Gestion des Collisions Critiques

```python
if collision_intensity > 75.0:
    self.critical_collision = True
    
# Dans main_V6:
if env.critical_collision:
    safe_gpu_cleanup()
    # Option: forcer done = True
```

### 6.3 Anti-Immobilit√©

```python
# Dans step():
if current_speed < 1.0:  # m/s
    control.throttle = 0.5
    control.brake = 0.0
```

**Probl√®me adress√©** : Le mod√®le peut apprendre √† rester immobile pour √©viter les p√©nalit√©s

### 6.4 Sauvegarde Incr√©mentale

```python
if episode % 3 == 0:
    save_model(model, optimizer, episode)
```

**Avantages** :
- Reprendre l'entra√Ænement apr√®s crash
- Tester diff√©rentes checkpoints
- Analyse de la convergence

### 6.5 Normalisation des Avantages

```python
advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)
```

**Effet** : Stabilise l'entra√Ænement en normalisant la magnitude des gradients

---

## 7. Hyperparam√®tres Cl√©s

### 7.1 R√©seau de Neurones

| Param√®tre | Valeur | Justification |
|-----------|--------|---------------|
| hidden_dim | 256 | Capacit√© suffisante pour 39 inputs |
| learning_rate | 3e-4 | Standard pour PPO |
| xavier_gain | 0.1 | Initialisation conservatrice |
| log_std_init | 0.5 | std ‚âà 1.65, exploration mod√©r√©e |

### 7.2 PPO

| Param√®tre | Valeur | R√¥le |
|-----------|--------|------|
| clip_eps | 0.2 | Limite les mises √† jour de policy |
| value_coef | 0.5 | Poids de la value loss |
| entropy_coef | 0.05 | Encourage l'exploration |
| epochs | 10 | R√©utilisation des donn√©es |
| Œ≥ (gamma) | 0.99 | Discount factor |
| Œª (lambda) | 0.95 | GAE parameter |

### 7.3 Environnement

| Param√®tre | Valeur | Description |
|-----------|--------|-------------|
| lidar_sectors | 32 | R√©solution angulaire |
| lidar_range | 50m | Port√©e du capteur |
| max_steps | 1000 | Limite par √©pisode |
| timeout | 5000 | Timeout en steps |
| min_ppo_buffer | 256 | Taille min pour update |

---

## 8. Probl√®mes Connus et Solutions

### 8.1 Incoh√©rence Sigmoid/Tanh

**Probl√®me** :
```python
# act() utilise sigmoid
action = torch.sigmoid(raw_action)

# evaluate() utilise atanh (inverse de tanh)
raw_action = torch.atanh(...)  # ERREUR!
```

**Solution** :
```python
# Choisir une transformation et l'inverse correcte:

# Option A: Tanh (recommand√© pour actions [-1, 1])
action = torch.tanh(raw_action)
raw_action = torch.atanh(torch.clamp(action, -0.99, 0.99))

# Option B: Sigmoid (actuel, pour actions [0, 1])
action = torch.sigmoid(raw_action)
raw_action = torch.logit(torch.clamp(action, 1e-7, 1-1e-7))
```

### 8.2 Immobilit√© Persistante

**Sympt√¥me** : Le v√©hicule reste bloqu√©

**Causes** :
1. P√©nalit√© de vitesse trop forte
2. R√©compense de lane keeping domine
3. Throttle non forc√©

**Solutions impl√©ment√©es** :
- Force throttle = 0.5 si vitesse < 1 m/s
- P√©nalit√© d'immobilit√© progressive
- Speed reward conditionnel au lane keeping

### 8.3 Explosions de Gradient

**Sympt√¥me** : Loss NaN, comportement erratique

**Solutions** :
- Normalisation des avantages
- Clipping PPO (ratio ‚àà [0.8, 1.2])
- Reward clipping ([-150, 150])
- Learning rate mod√©r√© (3e-4)

---

## 9. Recommandations d'Am√©lioration

### 9.1 Court Terme

1. **Corriger l'incoh√©rence sigmoid/atanh**
2. **Curriculum Learning** : Commencer sans obstacles
3. **Replay Buffer** : Stocker les meilleures trajectoires
4. **Wandb Integration** : Tracking avanc√©

### 9.2 Moyen Terme

1. **Multi-Task Learning** : Plusieurs destinations
2. **Attention Mechanism** : Pour le LIDAR
3. **Recurrent Policy** : LSTM pour m√©moire temporelle
4. **Domain Randomization** : Varier m√©t√©o, trafic, maps

### 9.3 Long Terme

1. **Vision-Based Policy** : Cam√©ra + CNN
2. **Hierarchical RL** : High-level (navigation) + Low-level (control)
3. **Transfer Learning** : Sim-to-Real
4. **Multi-Agent** : Interaction avec autres v√©hicules

---

## 10. Annexes

### 10.1 Commandes CARLA Utiles

```bash
# Lancer avec param√®tres custom
./CarlaUE4.sh -quality-level=Low -windowed -ResX=800 -ResY=600

# Mode spectateur
./CarlaUE4.sh -carla-port=2000 -opengl

# Logs
tail -f CarlaUE4/Saved/Logs/CarlaUE4.log
```

### 10.2 Debugging PyTorch

```python
# Gradients
for name, param in model.named_parameters():
    if param.grad is not None:
        print(f"{name}: {param.grad.norm()}")

# Watchdog NaN
torch.autograd.set_detect_anomaly(True)

# Profiling
with torch.profiler.profile() as prof:
    ppo_update(...)
print(prof.key_averages().table())
```

### 10.3 Formules Cl√©s

**Entropy** :
```
H = -Œ£ œÄ(a|s) log œÄ(a|s)
Pour Gaussian: H = 0.5 log(2œÄe œÉ¬≤)
```

**KL Divergence (Gaussians)** :
```
KL(œÄ_old || œÄ_new) = log(œÉ_new/œÉ_old) + (œÉ_old¬≤ + (Œº_old - Œº_new)¬≤)/(2œÉ_new¬≤) - 0.5
```

**PPO Clipping** :
```
r(Œ∏) = œÄ_Œ∏(a|s) / œÄ_Œ∏_old(a|s)
L^CLIP(Œ∏) = ùîº[min(r(Œ∏)A, clip(r(Œ∏), 1-Œµ, 1+Œµ)A)]
```

---

## Conclusion

Ce projet d√©montre une impl√©mentation compl√®te d'un syst√®me de conduite autonome par RL. Les points forts incluent un syst√®me de r√©compense sophistiqu√©, une gestion robuste de l'environnement CARLA, et une architecture PPO standard mais efficace. Les axes d'am√©lioration prioritaires sont la correction de l'incoh√©rence sigmoid/tanh et l'impl√©mentation d'un curriculum learning pour acc√©l√©rer la convergence.
